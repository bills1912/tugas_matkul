---
title: "APG Tugas Pertemuan 11"
author: "Bill Van Ricardo Zalukhu"
date: '2022-05-12'
output: html_document
---

```{r library, message=FALSE, warning=FALSE}
library(factoextra)
library(dendextend)
library(FactoClass)
library(openxlsx)
library(dplyr)
library(cluster)
library(plotly)
```

### 12.8
```{r}
D <- matrix(c(0, 9, 3, 6, 11, 9, 0, 7, 5, 10, 3, 7, 0, 9, 2, 6, 5, 9, 0, 8, 11, 10, 2, 8, 0),
            nrow = 5, ncol = 5)
as.dist(D)
```

Klasterisasi $average \space linkage$ menggunakan rataan dari jarak antara titik yang akan dikelompokan, sesuai dengan formula dari Johnson:
$$d_{(UV)W} = \displaystyle \frac{\sum_{i}\sum_{j} d_{ij}}{N_{(UV)}N_{W}}$$

dengan menggunakan formula di atas, proses pengklasterisasian adalah sebagai berikut:

- deteksi terlebih dahulu, dari matriks distance yang telah diberikan, mana variabel yang memiliki jarak paling kecil, dalam hal ini adalah `jarak antara variabel 3 dan 5` (distance nya sebesar 2). Dengan begitu, kita mendapatkan 1 kelompok data yang baru, yaitu kelompok variabel 3 dan 5

- lakukan penghitungan ulang jarak antara titik-titik selain 3 dan 5 dengan kelompok data baru yang telah kita peroleh sebelumnya (kelompok data 3 dan 5). Berikut perhitungannya:

```{r}
d_35_to_1 <- (D[3, 1] + D[5, 1])/2
d_35_to_2 <- (D[3, 2] + D[5, 2])/2
d_35_to_4 <- (D[3, 4] + D[5, 4])/2

d_35_to_1
d_35_to_2
d_35_to_4
```

dari hasi lperhitungan di atas, diperoleh nilai $d_{(35)1} = 7$, $d_{(35)2} = 8.5$, $d_{(35)3} = 8.5$, sehingga didapatkan matriks baru:

```{r}
D_35 <- matrix(c(0, 9, 6, 7, 9, 0, 5, 8.5, 6, 5, 0, 8.5, 7, 8.5, 8.5, 0), 
               nrow = 4, ncol = 4)
colnames(D_35) <- c("1", "2", "4", "3&5")
rownames(D_35) <- c("1", "2", "4", "3&5")
D_35
```

- selanjutnya, lakukan proses yang sama pada matriks baru tersebut. Terlebih dahulu, deteksi nilai jarak yang paling dekat. Jika dilhat, jarak yang paling dekat adalah antara variabel 2 dan 4, sehingga kelompok data yang baru diperoleh kembali adalah kelompok data 2 dan 4.

- selanjutnya, lakukan hal yang sama dengan proses sebelumnya, yaitu mencari jarak terhadap kelompok baru tersebut
```{r}
d_24_to_1 <- (D_35[2, 1] + D_35[3, 1])/2
d_24_to_35 <- (D_35[4, 2] + D_35[4, 3])/2

d_24_to_1
d_24_to_35
```

dari hasi lperhitungan di atas, diperoleh nilai $d_{(24)1} = 7.5$ dan $d_{(24)35} = 8.5$, sehingga didapatkan matriks baru:

```{r}
D_24 <- matrix(c(0, 7.5, 7, 7.5, 0, 8.5, 7, 8.5, 0), 
               nrow = 3, ncol = 3)
colnames(D_24) <- c("1", "2&4", "3&5")
rownames(D_24) <- c("1", "2&4", "3&5")
D_24
```

- selanjutnya, lakukan proses yang sama kembali pada matriks baru tersebut. Terlebih dahulu, deteksi nilai jarak yang paling dekat. Jika dilhat, jarak yang paling dekat adalah antara variabel 1 dan kelompok 3,5, sehingga kelompok data yang baru diperoleh kembali adalah kelompok data 1, 3, dan 5.

- selanjutnya, lakukan hal yang sama dengan proses sebelumnya, yaitu mencari jarak terhadap kelompok baru tersebut:
```{r}
d_135_to_24 <- (D_24[2, 1] + D_24[3, 2])/2

d_135_to_24
```

dari hasi lperhitungan di atas, diperoleh nilai $d_{(24)1} = 7.5$ dan $d_{(24)35} = 7.25$, sehingga didapatkan matriks baru:

```{r}
D_135 <- matrix(c(0, 8, 8, 0), 
               nrow = 2, ncol = 2)
colnames(D_135) <- c("1&3&5", "2&4")
rownames(D_135) <- c("1&3&5", "2&4")
D_135
```

- Untuk klaster berikutnya, adalah gabungan semua variabel, dengan jarak sebesar 8

Jika perhitungan di atas dimuat dalam dendogram, hasilnya adalah sebagai berikut:
```{r}
hc_data <- hclust(as.dist(D), method = "average")
fviz_dend(hc_data, cex = 0.8,
          main = "Dendrogram - Average Linkage",
          k = 2,
          k_colors = c("#2E9FDF", "#FC4E07"),
          color_labels_by_k = T,
          rect = T,
          xlab = "Objects", ylab = "Distance", sub = "")
```

Dari hasil dendrogram di atas, secara visual, dapat kita lihat bahwa jumlah optimal dari klaster yang terbentuk tanpa adanya outlier adalah sebanyak 2 klaster.


### 12.10
```{r}
X_dat <- c(2, 1, 5, 8)
```

a. Menghitung ESS
Untuk menghitung ESS, formula yang digunakan yaitu:
$$ESS = \sum_{j=1}^n (x_{j} - \bar{x})'(x_{j} - \bar{x})$$

Dengan menggunakan rumus tersebut, kita akan membuktikan jika nilai dari ESS dari data = 0.
```{r}
x_bar <- colMeans(t(X_dat))
for(i in c(1:4)){
  print(sum(t(X_dat[i] - x_bar[i]) %*% (X_dat[i] - x_bar[i])))
}
```
Dari hasil di atas, terbukti bahwa ESS untuk klaster `{1}, {2}, {3}, dan {4}` semuanya bernilai 0.

b. Menghitung ESS per klaster yang telah ditentukan pada soal
```{r}
ess_13_2_4 <- ((X_dat[1] - mean(X_dat[c(1, 3)]))^2 + (X_dat[3] - mean(X_dat[c(1, 3)]))^2) + 
              (X_dat[2] - mean(X_dat[2]))^2 + (X_dat[4] - mean(X_dat[4]))^2

ess_14_2_3 <- ((X_dat[1] - mean(X_dat[c(1, 4)]))^2 + (X_dat[4] - mean(X_dat[c(1, 4)]))^2) + 
              (X_dat[2] - mean(X_dat[2]))^2 + (X_dat[3] - mean(X_dat[3]))^2

ess_1_23_4 <- (X_dat[1] - mean(X_dat[1]))^2 + ((X_dat[2] - mean(X_dat[c(2, 3)]))^2 + 
              (X_dat[3] - mean(X_dat[c(2, 3)]))^2) + (X_dat[4] - mean(X_dat[4]))^2

ess_1_24_3 <- (X_dat[1] - mean(X_dat[1]))^2 + ((X_dat[2] - mean(X_dat[c(2, 4)]))^2 + 
              (X_dat[4] - mean(X_dat[c(2, 4)]))^2) + (X_dat[3] - mean(X_dat[3]))^2

ess_1_2_34 <- (X_dat[1] - mean(X_dat[1]))^2 + (X_dat[2] - mean(X_dat[2]))^2 + 
              ((X_dat[3] - mean(X_dat[c(3, 4)]))^2 + (X_dat[4] - mean(X_dat[c(3, 4)]))^2)

ess <- rbind(ess_13_2_4, ess_14_2_3, ess_1_23_4, ess_1_24_3, ess_1_2_34)
ess
```

c. Membuat Dendogram dari Hasil Pengklasteran Pada Step Kedua
```{r}
hc_ward <- hclust(dist(X_dat), "ward.D2")
fviz_dend(hc_ward, cex = 0.8,
          main = "Dendrogram - Ward's Method",
          k = 2,
          k_colors = c("#2E9FDF", "#FC4E07"),
          color_labels_by_k = T,
          rect = T,
          xlab = "Objects", ylab = "Distance", sub = "")
```

Dari hasil di atas, jika kita melihat hasil dendrogram yang terbentuk, ternyata jumlah klaster optimal yang terbentuk yaitu ada 2 cluster, ${12}$ dan ${34}$.

### 12.12
```{r}
items <- c("A", "B", "C", "D")
x1 <- c(5, -1, 1, -3)
x2 <- c(3, 1, -2, -2)
obs <- cbind(as.numeric(x1), as.numeric(x2))
as.data.frame(obs)
str(obs)
```
Jika kita menggambarkan scatter plot datanya, hasilnya:
```{r}
plot(obs, xlab = "x1", ylab = "x2", pch = items)
```

Jika kita lihat secara visual, terlihat bahwa titik items B, C, dan D seperti mengelompok menjadi 1 kelompok klaster yang sama, dilihat dari jarak antar titiknya yang saling berdekatan dan juga lokasi dari ketiga titik ini yang seperti berada pada 1 titik pusat tertentu, membentuk wilayah klaster baru. Sedangkan untuk titik A, seperti memisahkan diri dengan wilayah titik B, C, dan D, sehingga secara visual, titik A membentuk klaster baru sendiri. Kemungkinan juga, klaster A ini dapat terindikasi sebagai outlier, tetapi harus dibuktikan kebenarannya dengan bantuan analisis yang berkaitan dengan deteksi outlier di luar analisis klaster ini.

Selanjutnya, kita akan menghitung inisialisasi dari centroid cluster yang akan dibangung dengan kombinasi pengelompokan $(AC)$ dan $(BD)$

```{r}
AC_xbar1 <- (obs[1, 1] + obs[3, 1])/2
AC_xbar2 <- (obs[1, 2] + obs[3, 2])/2
BD_xbar1 <- (obs[2, 1] + obs[4, 1])/2
BD_xbar2 <- (obs[2, 2] + obs[4, 2])/2

cent <- matrix(c(AC_xbar1, BD_xbar1, AC_xbar2, BD_xbar2), nrow = 2, ncol = 2)
colnames(cent) <- c("xbar1", "xbar2")
rownames(cent) <- c("(AC)", "(BD)")
cent
```

Selanjutnya, kita akan menghitung jarak antara titik A dan B ke centroid yang telah dihitung sebelumnya menggunakan metode Eucledian Distance:
```{r}
d2_A_AC <- (obs[1, 1] - cent[1, 1])^2  + (obs[1, 2] - cent[1, 2])^2
d2_A_BD <- (obs[1, 1] - cent[2, 1])^2  + (obs[1, 2] - cent[2, 2])^2
d2_B_AC <- (obs[2, 1] - cent[1, 1])^2  + (obs[2, 2] - cent[1, 2])^2
d2_B_BD <- (obs[2, 1] - cent[2, 1])^2  + (obs[2, 2] - cent[2, 2])^2
d2_C_AC <- (obs[3, 1] - cent[1, 1])^2  + (obs[3, 2] - cent[1, 2])^2
d2_C_BD <- (obs[3, 1] - cent[2, 1])^2  + (obs[3, 2] - cent[2, 2])^2
d2_D_AC <- (obs[4, 1] - cent[1, 1])^2  + (obs[4, 2] - cent[1, 2])^2
d2_D_BD <- (obs[4, 1] - cent[2, 1])^2  + (obs[4, 2] - cent[2, 2])^2

D2 <- matrix(c(d2_A_AC, d2_B_AC, d2_C_AC, d2_D_AC, d2_A_BD, d2_B_BD, d2_C_BD, d2_D_BD),
             nrow = 4, ncol = 2)
colnames(D2) <- c("(AC)", "(BD)")
rownames(D2) <- c("A", "B", "C", "D")
as.data.frame(D2)
```

Dari hasil di atas, ternyata cluster yang terbentuk adalah cluster yang sama dengan inisiasi awal, yaitu $(AC)$ dan $(BD)$, sehingga untuk jarak antar items dengan centroidnya tetap sama dengan perhitungan yang sebelumnya
Dari hasil tersebut, untuk pemilihan pembentukan cluster nya, tergantung dari subjektifitas dan objektifitas dari peneliti sendiri, ingin memilih pembentukan cluster dengan centroid jenis apa. Hal yang perlu dipertimbangkan adalah dengan penggunaan centroid yang dipilih dalam pembentukan cluster, peneliti harus melihat dari sisi jarak dan juga sisi ketumpangtindihan cluster yang dibentuk dari centroid tersebut, agar hasil cluster lebih baik dan informasi yang didapatkan lebih optimal.

### 12.16
```{r}
nat_rec <- read.xlsx("C:/Users/bilva/OneDrive/Documents/3SI1/Semester 6/Tugas Matkul/tugas_matkul/Analisis Peubah Ganda/Tugas 10-221911069-3SI1-Bill Van Ricardo Zalukhu/National Track Records for Women.xlsx")

for(i in c(2:8)){
  nat_rec[, i] <- as.numeric(nat_rec[, i])
}

```

a. Menghitung Matriks Distance dengan Euclidian
```{r}
D_nat_rec <- dist(nat_rec[, c(2:8)], method = "euclidian")
```

b. Membuat Hierarchical Cluster dengan Metode Single Linkage dan Complete Linkage
```{r}
hc_single <- hclust(D_nat_rec, method = "single")
fviz_dend(hc_single, cex = 0.5,
          main = "Dendrogram - Single Linkage from Raw Dataset",
          k = 4,
          k_colors = c("#2E9FDF", "#00ff00", "#E7B800", "#FC4E07"),
          color_labels_by_k = T,
          rect = T,
          xlab = "Objects", ylab = "Distance", sub = "")
```

```{r}
hc_complete <- hclust(D_nat_rec, method = "complete")
fviz_dend(hc_complete, cex = 0.5,
          main = "Dendrogram - Complete Linkage from Raw Dataset",
          k = 6,
          k_colors = c("#cc33ff", "#00AFBB", "#E7B800", "#FC4E07", "#00ff00", "#3399ff"),
          color_labels_by_k = T,
          rect = T,
          xlab = "Objects", ylab = "Distance", sub = "")
```

Dari hasil klasterisasi di atas, dapat kita lihat bahwa untuk metode single linkage hierarchical cluster, jumlah klaster optimal yang bisa dibentuk menggunakan metode ini adalah sebanyak 4 klaster, dilihat dari kedekatan dan pengelompokan secara subjektif terhadpa dendogram nya. Lain halnya dengan complete linkage hierarchical cluster, secara subjektif dan visual, dapat kita lihat bahwa jumlah optimum dari klaster yang dapat dibentuk yaitu sebanyak 6 klaster. Dan jika bandingkan antara hasil dari single linkage dengan complete linkage, terlihat bahwa hasil dari complete linkage lebih baik dibandingkan dengan single linkage karena terlihat pada single linkage, lebih banyak cluster yang terindikasi sebagai pencilan cluster dibanding dengan complete linkage yang cluster nya sudah terlihat lebih optimum.

c. Membuat Non-Hierarchical Cluster dengan K-Means Method
```{r}
fviz_nbclust(nat_rec[, c(2:8)], kmeans, method = "wss")
```

Dari diagram di atas, kita bisa menentukan kandidat jumlah klaster yang bisa kita bentuk dari data yang digunakan, yaitu antara 2 klaster dan/atau 3 klaster.

- Untuk 2 klaster:
```{r}
km_nat_rec <- kmeans(nat_rec[, c(2:8)], centers = 2, iter.max = 1000)
fviz_cluster(km_nat_rec, data = as.data.frame(nat_rec[, c(2:8)]))
```

- Untuk 3 klaster:
```{r}
km_nat_rec <- kmeans(nat_rec[, c(2:8)], centers = 3)
fviz_cluster(km_nat_rec, data = as.data.frame(nat_rec[, c(2:8)]))
```

Dari hasil di atas, secara natural dan dengan menggunakan metode k-means, klaster yang dapat dibentuk yaitu sebanyak 2 atau 3 klaster. Tetapi jika dilihat secar visual, baik itu 2 maupun 3 klaster, ternyata masih terjadi overlapping diantara klaster yang terbentuk. Untuk 2 klaster, tingkat overlapping nya masih tidak terlalu luas, tetapi untuk 3 klaster, overlapping yang terjadi dapat dikatakan buruk, terlihat dari overlapping yang terjadi antara klaster berwarna `merah` dengan klaster yang berwarna `biru`, dimana bahkan, centroid dari klaster berwarna `biru` ter-overlap dengan klaster berwarna `merah`. Hal ini dianggap kurang baik karena bisa saja oleh karena kejadian overlapping tersebut, peneliti bisa kehilangan informasi yang penting dari hasil pengklasterannya.
Hal ini dapat diakibatkan oleh banyak hal, beberapa diantaranya adalah skala antar data yang jauh (data bervariasi), adanya pencilan, dan hal-hal lain yang dapat mempengaruhi hasil pengklasteran.

Karena hasil klasternya kurang baik, dilakukan penormalisasian data yang dianggap salah satu cara yang efektif untuk menghasilkan klaster yang baik, dari segi jumlah dan visual.
Jika datanya distandarisasikan, maka hasilnya akan seperti berikut:
```{r}
nat_rec_norm <- nat_rec[, c(2:8)] %>% mutate_all(~(scale(.) %>% as.vector))
```

```{r}
D_nat_rec_norm <- dist(nat_rec_norm, method = "euclidian")
```

Untuk hierarchical single linkage cluster:
```{r}
hc_single_norm <- hclust(D_nat_rec_norm, method = "single")
fviz_dend(hc_single_norm, cex = 0.5,
          main = "Dendrogram - Single Linkage from Normalize Dataset",
          k = 6,
          k_colors = c("#2E9FDF", "#00ff00", "#E7B800", "#FC4E07", "#3399ff", "#00AFBB"),
          color_labels_by_k = T,
          rect = T,
          xlab = "Objects", ylab = "Distance", sub = "")
```

Untuk hierarchical complete linkage cluster:
```{r}
hc_complete_norm <- hclust(D_nat_rec_norm, method = "complete")
fviz_dend(hc_complete_norm, cex = 0.5,
          main = "Dendrogram - Complete Linkage from Normalize Dataset",
          k = 4,
          k_colors = c("#cc33ff", "#00AFBB", "#E7B800", "#FC4E07", "#00ff00", "#3399ff"),
          color_labels_by_k = T,
          rect = T,
          xlab = "Objects", ylab = "Distance", sub = "")
```

Dari hasil klasterisasi di atas, setelah datanya distandarkan, dapat kita lihat bahwa untuk metode single linkage hierarchical cluster, jumlah klaster optimal yang bisa dibentuk menggunakan metode ini adalah sebanyak 6 klaster, dilihat dari kedekatan dan pengelompokan secara subjektif terhadpa dendogram nya. Lain halnya dengan complete linkage hierarchical cluster, secara subjektif dan visual, dapat kita lihat bahwa jumlah optimum dari klaster yang dapat dibentuk yaitu sebanyak 4 klaster (hasilnya merupakan kebalikan dari hasil sebelumnya, dimana datanya masih belum distandarkan).
Dan jika bandingkan antara hasil dari single linkage dengan complete linkage, terlihat bahwa hasil dari complete linkage lebih baik dibandingkan dengan single linkage karena terlihat pada single linkage, lebih banyak cluster yang terindikasi sebagai pencilan cluster dibanding dengan complete linkage yang cluster nya sudah terlihat lebih optimum.

Untuk K-Means clustering:
```{r}
fviz_nbclust(nat_rec_norm, kmeans, method = "wss")
```

Dari plot di atas, keliatan jelas bahwa elbow atau titik pembelokan yang menjadi jumlah optimum klasternya jatuh pada titik dengan jumlah 3 klaster, maka:
```{r}
km_nat_rec <- kmeans(nat_rec_norm, centers = 3)
fviz_cluster(km_nat_rec, data = as.data.frame(nat_rec_norm))
```

Dapat dilihat perubahan total dari hasil klasterisasi nya menggunakan k-means. Hasil dari klaster setelah dinormalisasi ternyata memberikan bentuk dan rupa klaster yang lebih baik, tidak saling tumpang tindih antar sesama klaster. Ini dapat mengindikasikan bahwa hasil dari klaster dari data ternormalisasi lebih baik dibanding dengan klaster dari data yang belum ternormalisai. Dapat dilihat dari hasilnya, jumlah optimum dari klaster yaitu sebanyak 3 klaster, dan setelah dibentuk, ternyata hasilnya sudah baik juga, dari segi visual dan pengelompokannya.




